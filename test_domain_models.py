"""
Test de validaci√≥n - Modelos de Dominio Fase 1

Verifica que los modelos de dominio funcionan correctamente
y que el c√≥digo existente no se rompe.
"""

import sys
from pathlib import Path
from shutil import copyfile

# Agregar path de daia al PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent))

from daia.domain.models import (
    # Core models
    AuditedCall,
    AuditResult,
    Finding,
    Metric,
    # Enums
    CallStatus,
    ServiceLevel,
    FindingSeverity,
    FindingCategory,
    MetricType,
    MetricCategory,
    MetricStatus,
    # Factories
    create_new_call,
    create_qa_score_metric,
    create_compliance_finding,
    create_completed_result,
)

from datetime import datetime


def ensure_test_audio():
    """Garantiza que el audio de prueba exista para los tests."""
    target = Path("audio_in/Grabaci√≥n de llamada 1929_251211_145452.m4a")
    if target.exists():
        return

    candidates = [
        Path("audio_in/Grabaci√≥n de llamada 097748960_260115_102922.m4a"),
        Path("audio_in/llamada1.m4a"),
        Path("audio_in/llamada2.m4a"),
    ]
    source = next((c for c in candidates if c.exists()), None)
    if source:
        target.parent.mkdir(parents=True, exist_ok=True)
        copyfile(source, target)
    else:
        raise FileNotFoundError("No se encontr√≥ audio de prueba en audio_in/")


def test_audited_call():
    """Test: AuditedCall model"""
    ensure_test_audio()
    print("\n" + "="*70)
    print("TEST: AuditedCall")
    print("="*70)
    
    # Crear llamada nueva
    call = create_new_call(
        filename="test_call.wav",
        audio_path="audio_in/Grabaci√≥n de llamada 1929_251211_145452.m4a",
        service_level=ServiceLevel.STANDARD
    )
    
    print(f"‚úì Llamada creada: {call}")
    print(f"  - Status: {call.status.value}")
    print(f"  - Level: {call.service_level.value}")
    print(f"  - Requires standard: {call.requires_standard_analysis()}")
    
    assert call.call_id is None
    assert call.status == CallStatus.PENDING
    assert call.service_level == ServiceLevel.STANDARD
    
    print("‚úÖ AuditedCall: OK")


def test_finding():
    """Test: Finding model"""
    print("\n" + "="*70)
    print("TEST: Finding")
    print("="*70)
    
    # Crear hallazgo de cumplimiento
    finding = create_compliance_finding(
        title="Saludo inicial no ejecutado",
        description="El agente no salud√≥ al cliente al inicio de la llamada",
        severity=FindingSeverity.HIGH,
        evidence="Cliente: Hola... [silencio] | Agente: S√≠, d√≠game",
        recommendation="Reforzar protocolo de saludo inicial"
    )
    
    print(f"‚úì Finding creado: {finding}")
    print(f"  - Severidad: {finding.severity.value}")
    print(f"  - Requiere acci√≥n: {finding.requires_action}")
    print(f"  - Es cr√≠tico: {finding.is_critical}")
    
    assert finding.category == FindingCategory.COMPLIANCE
    assert finding.severity == FindingSeverity.HIGH
    assert finding.requires_action
    assert not finding.is_critical
    
    print("‚úÖ Finding: OK")


def test_metric():
    """Test: Metric model"""
    print("\n" + "="*70)
    print("TEST: Metric")
    print("="*70)
    
    # Crear m√©trica de QA
    metric = create_qa_score_metric(score=75.5)
    
    print(f"‚úì M√©trica creada: {metric}")
    print(f"  - Valor formateado: {metric.formatted_value}")
    print(f"  - Status: {metric.status.value if metric.status else 'N/A'}")
    print(f"  - Dentro de rango: {metric.is_within_acceptable_range}")
    
    assert metric.name == "qa_score"
    assert metric.value == 75.5
    assert metric.metric_type == MetricType.PERCENTAGE
    assert metric.is_within_acceptable_range
    
    print("‚úÖ Metric: OK")


def test_audit_result():
    """Test: AuditResult (aggregate)"""
    print("\n" + "="*70)
    print("TEST: AuditResult")
    print("="*70)
    
    # Crear llamada completada
    from daia.domain.models import create_completed_call
    
    call = create_completed_call(
        call_id=1,
        filename="test_call.wav",
        audio_path="audio_in/Grabaci√≥n de llamada 1929_251211_145452.m4a",
        duration_seconds=180.0,
        service_level=ServiceLevel.STANDARD
    )
    
    # Crear m√©tricas
    metrics = [
        create_qa_score_metric(score=82.0),
        Metric(
            name="call_duration",
            value=180.0,
            metric_type=MetricType.SECONDS,
            category=MetricCategory.PERFORMANCE,
            unit="s"
        )
    ]
    
    # Crear findings
    findings = [
        create_compliance_finding(
            title="Protocolo incompleto",
            description="Falt√≥ confirmaci√≥n final",
            severity=FindingSeverity.MEDIUM,
            recommendation="Incluir confirmaci√≥n en cierre"
        )
    ]
    
    # Crear resultado completo
    result = create_completed_result(
        audited_call=call,
        findings=findings,
        metrics=metrics,
        transcript_text="Esta es la transcripci√≥n de prueba...",
        processing_time_seconds=45.2
    )
    
    print(f"‚úì Resultado creado: {result}")
    print(f"  - Status general: {result.overall_status}")
    print(f"  - QA Score: {result.qa_score}%")
    print(f"  - Total findings: {result.total_findings}")
    print(f"  - Total m√©tricas: {result.total_metrics}")
    print(f"  - ¬øAprueba?: {result.is_passing}")
    print(f"  - ¬øRequiere revisi√≥n?: {result.requires_review}")
    
    # Validaciones
    assert result.audited_call.is_completed
    assert result.qa_score == 82.0
    assert result.total_findings == 1
    assert result.total_metrics == 2
    assert result.is_passing
    
    # Resumen
    summary = result.summary_dict()
    print(f"\n  Resumen:")
    for key, value in summary.items():
        print(f"    - {key}: {value}")
    
    print("‚úÖ AuditResult: OK")


def test_business_rules():
    """Test: Validaciones de reglas de negocio"""
    print("\n" + "="*70)
    print("TEST: Business Rules")
    print("="*70)
    
    # Test 1: Percentage debe estar entre 0-100
    try:
        bad_metric = Metric(
            name="bad_percentage",
            value=150.0,  # Inv√°lido!
            metric_type=MetricType.PERCENTAGE,
            category=MetricCategory.QUALITY
        )
        print("‚ùå ERROR: Deber√≠a haber fallado con percentage > 100")
        assert False
    except ValueError as e:
        print(f"‚úì Validaci√≥n correcta: {e}")
    
    # Test 2: Finding title debe ser <= 100 chars
    try:
        bad_finding = Finding(
            category=FindingCategory.QUALITY,
            severity=FindingSeverity.LOW,
            title="A" * 101,  # Demasiado largo!
            description="Test"
        )
        print("‚ùå ERROR: Deber√≠a haber fallado con title > 100 chars")
        assert False
    except ValueError as e:
        print(f"‚úì Validaci√≥n correcta: {e}")
    
    # Test 3: CRITICAL findings deben tener recomendaci√≥n
    try:
        bad_finding = Finding(
            category=FindingCategory.RISK,
            severity=FindingSeverity.CRITICAL,
            title="Critical issue",
            description="Very bad",
            recommendation=None  # Falta!
        )
        print("‚ùå ERROR: Deber√≠a requerir recommendation para CRITICAL")
        assert False
    except ValueError as e:
        print(f"‚úì Validaci√≥n correcta: {e}")
    
    print("‚úÖ Business Rules: OK")


def test_backward_compatibility():
    """Test: El c√≥digo existente sigue funcionando"""
    print("\n" + "="*70)
    print("TEST: Backward Compatibility")
    print("="*70)
    
    # Importar m√≥dulos existentes
    try:
        sys.path.insert(0, str(Path(__file__).parent / "scripts"))
        
        from lib_resources import ConfigManager
        from pipeline import PipelineOrchestrator
        
        print("‚úì M√≥dulos existentes importados correctamente")
        
        # Verificar que config carga
        config = ConfigManager("config.yaml")
        print(f"‚úì ConfigManager funciona")
        
        print("‚úÖ Backward Compatibility: OK")
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Warning: {e}")
        print("   (Esto es normal si no est√° en el entorno de ejecuci√≥n)")


def main():
    """Ejecutar todos los tests"""
    print("\n" + "="*70)
    print("üß™ VALIDACI√ìN DE MODELOS DE DOMINIO - FASE 1")
    print("="*70)
    
    try:
        test_audited_call()
        test_finding()
        test_metric()
        test_audit_result()
        test_business_rules()
        test_backward_compatibility()
        
        print("\n" + "="*70)
        print("‚úÖ TODOS LOS TESTS PASARON")
        print("="*70)
        print("\nüì¶ Modelos de dominio listos para usar:")
        print("   - AuditedCall (entity)")
        print("   - AuditResult (aggregate)")
        print("   - Finding (value object)")
        print("   - Metric (value object)")
        print("\nüéØ Sistema listo para vender auditor√≠as")
        print("="*70)
        
        return 0
        
    except Exception as e:
        print("\n" + "="*70)
        print(f"‚ùå ERROR EN TESTS: {e}")
        print("="*70)
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
