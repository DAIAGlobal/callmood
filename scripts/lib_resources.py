"""
DAIA - Resource Management & Auto-Fallback System
Detecta recursos disponibles y configura autom√°ticamente los modelos.
100% Local, 0 USD, Control Total.
"""

import torch
import psutil
import logging
from typing import Tuple, Dict, Any
from pathlib import Path

logger = logging.getLogger(__name__)


class ResourceManager:
    """Gestor de recursos y auto-fallback inteligente"""
    
    def __init__(self):
        self.has_gpu = torch.cuda.is_available()
        self.gpu_name = torch.cuda.get_device_name(0) if self.has_gpu else None
        self.gpu_vram = self._get_gpu_memory()
        self.cpu_cores = psutil.cpu_count(logical=False)
        self.cpu_freq = psutil.cpu_freq().max if psutil.cpu_freq() else 0
        self.ram_total = psutil.virtual_memory().total / (1024**3)  # GB
        self.ram_available = psutil.virtual_memory().available / (1024**3)  # GB
        
        logger.info(f"GPU disponible: {self.has_gpu}")
        if self.has_gpu:
            logger.info(f"GPU: {self.gpu_name} ({self.gpu_vram:.1f}GB)")
        logger.info(f"CPU: {self.cpu_cores} cores @ {self.cpu_freq:.0f}MHz")
        logger.info(f"RAM: {self.ram_available:.1f}GB / {self.ram_total:.1f}GB")
    
    def _get_gpu_memory(self) -> float:
        """Obtiene VRAM disponible en GB"""
        if not self.has_gpu:
            return 0.0
        return torch.cuda.get_device_properties(0).total_memory / (1024**3)
    
    def get_whisper_model(self) -> str:
        """
        Selecciona autom√°ticamente el mejor modelo Whisper.
        
        Returns:
            str: 'large', 'medium', 'small'
        """
        # GPU: Seleccionar por VRAM disponible
        if self.has_gpu:
            if self.gpu_vram >= 10:
                logger.info("‚úì GPU con 10GB+: usando modelo 'large'")
                return "large"
            elif self.gpu_vram >= 5:
                logger.info("‚úì GPU con 5GB+: usando modelo 'medium'")
                return "medium"
            else:
                logger.info("‚úì GPU con <5GB: usando modelo 'small'")
                return "small"
        
        # CPU: Usar peque√±o por rendimiento
        logger.warning("‚ö† Sin GPU detectada: fallback a modelo 'small' (CPU mode)")
        logger.info("  Nota: El procesamiento ser√° m√°s lento en CPU")
        logger.info("  Configuraci√≥n CPU FP32 para compatibilidad")
        return "small"
    
    def get_whisper_config(self) -> Dict[str, Any]:
        """
        Obtiene configuraci√≥n optimizada para Whisper.
        
        Returns:
            dict: Configuraci√≥n Whisper optimizada
        """
        base_config = {
            "verbose": False,
            "task": "transcribe",
            "best_of": 1,
            "beam_size": 5,
            "patience": 1.0,
        }
        
        # Optimizaci√≥n seg√∫n disponibilidad de GPU
        if self.has_gpu:
            base_config["fp16"] = True  # FP16 en GPU acelera
            logger.info("‚úì FP16 habilitado para GPU")
        else:
            base_config["fp16"] = False  # FP32 en CPU (m√°s compatible)
            logger.info("‚úì FP32 habilitado para CPU (m√°xima compatibilidad)")
        
        return base_config
    
    def get_device(self) -> str:
        """
        Retorna el device optimizado para PyTorch.
        
        Returns:
            str: 'cuda' o 'cpu'
        """
        device = "cuda" if self.has_gpu else "cpu"
        logger.info(f"Device seleccionado: {device.upper()}")
        return device
    
    def get_batch_size(self) -> int:
        """
        Calcula batch size √≥ptimo basado en memoria disponible.
        
        Returns:
            int: Batch size recomendado
        """
        if self.has_gpu:
            # GPU: batch size basado en VRAM
            if self.gpu_vram >= 10:
                return 16
            elif self.gpu_vram >= 5:
                return 8
            else:
                return 4
        else:
            # CPU: conservative
            return 1
    
    def get_worker_threads(self, max_workers: int = 4) -> int:
        """
        Calcula n√∫mero √≥ptimo de workers para procesamiento paralelo.
        
        Args:
            max_workers: M√°ximo de workers configurado
            
        Returns:
            int: N√∫mero recomendado de workers
        """
        # No usar m√°s workers que cores disponibles
        available = min(self.cpu_cores, max_workers)
        logger.info(f"Workers paralelos: {available} (de {self.cpu_cores} cores)")
        return available
    
    def log_summary(self):
        """Imprime resumen de recursos disponibles"""
        print("\n" + "="*70)
        print("DAIA - RESUMEN DE RECURSOS DISPONIBLES")
        print("="*70)
        
        print(f"\nüñ•Ô∏è  GPU")
        if self.has_gpu:
            print(f"   ‚úì Disponible: {self.gpu_name}")
            print(f"   ‚úì VRAM: {self.gpu_vram:.1f}GB")
        else:
            print(f"   ‚úó No detectada (procesamiento en CPU)")
        
        print(f"\n‚öôÔ∏è  CPU")
        print(f"   ‚úì Cores: {self.cpu_cores}")
        print(f"   ‚úì Frecuencia: {self.cpu_freq:.0f}MHz")
        
        print(f"\nüíæ RAM")
        print(f"   ‚úì Total: {self.ram_total:.1f}GB")
        print(f"   ‚úì Disponible: {self.ram_available:.1f}GB")
        
        print(f"\nüìä CONFIGURACI√ìN RECOMENDADA")
        model = self.get_whisper_model()
        print(f"   ‚úì Modelo Whisper: {model}")
        print(f"   ‚úì Device: {self.get_device().upper()}")
        print(f"   ‚úì FP16: {'S√≠' if self.get_whisper_config().get('fp16') else 'No'}")
        print(f"   ‚úì Batch Size: {self.get_batch_size()}")
        print(f"   ‚úì Workers: {self.get_worker_threads()}")
        print("\n" + "="*70 + "\n")


class ConfigManager:
    """Gestor de configuraci√≥n YAML con validaci√≥n"""
    
    def __init__(self, config_path: str = "config.yaml"):
        import yaml
        
        self.config_path = Path(config_path)
        
        if not self.config_path.exists():
            raise FileNotFoundError(f"config.yaml no encontrado en {config_path}")
        
        with open(self.config_path, "r", encoding="utf-8") as f:
            self.config = yaml.safe_load(f)
        
        logger.info(f"‚úì Configuraci√≥n cargada desde {config_path}")
    
    def get(self, key: str, default=None):
        """Obtiene valor de configuraci√≥n por clave (ej: 'general.language')"""
        keys = key.split(".")
        value = self.config
        
        for k in keys:
            if isinstance(value, dict):
                value = value.get(k)
            else:
                return default
        
        return value if value is not None else default
    
    def get_pipeline_level(self, level: str = "standard") -> Dict[str, Any]:
        """Obtiene configuraci√≥n del nivel de pipeline"""
        valid_levels = ["basic", "standard", "advanced"]
        if level not in valid_levels:
            logger.warning(f"Nivel '{level}' inv√°lido, usando 'standard'")
            level = "standard"
        
        return self.config["pipeline"]["levels"][level]
    
    def get_qa_rules(self, level: str = "standard") -> Dict[str, Any]:
        """Obtiene reglas QA para un nivel espec√≠fico"""
        return self.config["qa"]["rules"][level]
    
    def validate(self) -> bool:
        """Valida la integridad de la configuraci√≥n"""
        required_sections = [
            "general", "transcription", "qa", "risk_analysis",
            "kpis", "database", "pipeline", "paths"
        ]
        
        for section in required_sections:
            if section not in self.config:
                logger.error(f"‚ùå Falta secci√≥n requerida: {section}")
                return False
        
        logger.info("‚úì Configuraci√≥n validada correctamente")
        return True


if __name__ == "__main__":
    # Logging setup
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # Test resource manager
    rm = ResourceManager()
    rm.log_summary()
    
    # Test config manager
    try:
        cm = ConfigManager("config.yaml")
        cm.validate()
    except FileNotFoundError as e:
        logger.error(f"Error: {e}")
